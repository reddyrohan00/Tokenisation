# ğŸ§  Tokenisation

A deep dive into text tokenization â€” the fundamental first step in any NLP pipeline. This repository contains various techniques and implementations to convert raw text into meaningful tokens for downstream processing.

---

## ğŸ“š Overview

This project explores:
- Word, subword, and character-level tokenization
- Tokenizers from popular NLP libraries: `NLTK`, `spaCy`, `Hugging Face`, etc.
- Custom tokenizers and whitespace handling
- Handling edge cases like punctuation, emojis, and special characters

---

## ğŸ› ï¸ Technologies Used

- Python ğŸ
- NLTK
- spaCy
- Hugging Face Tokenizers
- Regex (Regular Expressions)

---

## ğŸ“‚ Project Structure

