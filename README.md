# 🧠 Tokenisation

A deep dive into text tokenization — the fundamental first step in any NLP pipeline. This repository contains various techniques and implementations to convert raw text into meaningful tokens for downstream processing.

---

## 📚 Overview

This project explores:
- Word, subword, and character-level tokenization
- Tokenizers from popular NLP libraries: `NLTK`, `spaCy`, `Hugging Face`, etc.
- Custom tokenizers and whitespace handling
- Handling edge cases like punctuation, emojis, and special characters

---

## 🛠️ Technologies Used

- Python 🐍
- NLTK
- spaCy
- Hugging Face Tokenizers
- Regex (Regular Expressions)

---

## 📂 Project Structure

